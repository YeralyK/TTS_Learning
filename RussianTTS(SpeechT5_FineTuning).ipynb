{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMhWvBnSb3teGSPEHWk137",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeralyK/TTS_Learning/blob/main/RussianTTS(SpeechT5_FineTuning).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHuwzn638VGo"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets soundfile accelerate speechbrain==0.5.16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ImPcOyYDDCpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "#https://huggingface.co/datasets/0x7o/klara-voice\n",
        "dataset = load_dataset(\"0x7o/klara-voice\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "hCMwEc6nD2a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "F9hWiKF8EaBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "hhOkPE5RFxav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "half_size = len(dataset) // 3\n",
        "\n",
        "dataset = dataset.select(range(half_size))\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "SE0tSfCVFyuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "qjld8Fb6F97x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor\n",
        "\n",
        "checkpoint = \"microsoft/speecht5_tts\"\n",
        "processor = SpeechT5Processor.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "_Q0RhskJGal7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = processor.tokenizer"
      ],
      "metadata": {
        "id": "rTAilO3PGey4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2:5]"
      ],
      "metadata": {
        "id": "mlNKKH-zGmTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_chars(batch):\n",
        "    all_text = \" \".join(batch[\"text\"])\n",
        "    vocab = list(set(all_text))\n",
        "    return {\"audio\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "\n",
        "vocabs = dataset.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "dataset_vocab = set(vocabs[\"audio\"][0])\n",
        "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
      ],
      "metadata": {
        "id": "xDsFVka2Gzws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_vocab - tokenizer_vocab"
      ],
      "metadata": {
        "id": "N9t8j0r_HT0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
        "\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "def add_normalized_text(example):\n",
        "    example['normalized_text'] = normalize_text(example['text'])\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(add_normalized_text)\n",
        "\n",
        "print(dataset[2:5])"
      ],
      "metadata": {
        "id": "94Ztul4TH_AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_chars(batch):\n",
        "    all_text = \" \".join(batch[\"normalized_text\"])\n",
        "    vocab = list(set(all_text))\n",
        "    return {\"audio\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "\n",
        "vocabs = dataset.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "dataset_vocab = set(vocabs[\"audio\"][0])\n",
        "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
      ],
      "metadata": {
        "id": "7tE_Fc6sIr8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_vocab - tokenizer_vocab"
      ],
      "metadata": {
        "id": "XALWdIMZI35s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replacements = [\n",
        "    (\"а\", \"a\"),\n",
        "    (\"б\", \"b\"),\n",
        "    (\"в\", \"v\"),\n",
        "    (\"г\", \"g\"),\n",
        "    (\"д\", \"d\"),\n",
        "    (\"е\", \"e\"),\n",
        "    (\"ё\", \"yo\"),\n",
        "    (\"ж\", \"zh\"),\n",
        "    (\"з\", \"z\"),\n",
        "    (\"и\", \"i\"),\n",
        "    (\"й\", \"y\"),\n",
        "    (\"к\", \"k\"),\n",
        "    (\"л\", \"l\"),\n",
        "    (\"м\", \"m\"),\n",
        "    (\"н\", \"n\"),\n",
        "    (\"о\", \"o\"),\n",
        "    (\"п\", \"p\"),\n",
        "    (\"р\", \"r\"),\n",
        "    (\"с\", \"s\"),\n",
        "    (\"т\", \"t\"),\n",
        "    (\"у\", \"u\"),\n",
        "    (\"ф\", \"f\"),\n",
        "    (\"х\", \"kh\"),\n",
        "    (\"ц\", \"ts\"),\n",
        "    (\"ч\", \"ch\"),\n",
        "    (\"ш\", \"sh\"),\n",
        "    (\"щ\", \"shch\"),\n",
        "    (\"ъ\", \"\"),      # hard sign removed\n",
        "    (\"ы\", \"y\"),\n",
        "    (\"ь\", \"\"),      # soft sign removed\n",
        "    (\"э\", \"e\"),\n",
        "    (\"ю\", \"yu\"),\n",
        "    (\"я\", \"ya\"),\n",
        "]\n",
        "def cleanup_text(inputs):\n",
        "    text = inputs[\"normalized_text\"]\n",
        "    for src, dst in replacements:\n",
        "        text = text.replace(src, dst)\n",
        "    inputs[\"normalized_text\"] = text\n",
        "    return inputs\n",
        "\n",
        "dataset = dataset.map(cleanup_text)"
      ],
      "metadata": {
        "id": "DZJU7gB5I-OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "import huggingface_hub\n",
        "\n",
        "if not hasattr(huggingface_hub, \"_orig_hf_hub_download\"):\n",
        "    huggingface_hub._orig_hf_hub_download = huggingface_hub.hf_hub_download\n",
        "\n",
        "def _hf_hub_download_compat(repo_id, filename, *args, **kwargs):\n",
        "    if \"use_auth_token\" in kwargs and \"token\" not in kwargs:\n",
        "        kwargs[\"token\"] = kwargs.pop(\"use_auth_token\")\n",
        "    else:\n",
        "        kwargs.pop(\"use_auth_token\", None)\n",
        "\n",
        "    if repo_id == \"speechbrain/spkrec-xvect-voxceleb\" and filename == \"custom.py\":\n",
        "        stub_dir = os.path.join(\"/tmp\", \"speechbrain_hf_stubs\", \"speechbrain_spkrec_xvect\")\n",
        "        os.makedirs(stub_dir, exist_ok=True)\n",
        "        stub_path = os.path.join(stub_dir, \"custom.py\")\n",
        "        if not os.path.exists(stub_path):\n",
        "            with open(stub_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"# Stub file auto-created to satisfy SpeechBrain downloader.\\n\")\n",
        "        return stub_path\n",
        "\n",
        "    return huggingface_hub._orig_hf_hub_download(repo_id, filename, *args, **kwargs)\n",
        "\n",
        "huggingface_hub.hf_hub_download = _hf_hub_download_compat\n",
        "\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "speaker_model = EncoderClassifier.from_hparams(\n",
        "    source=spk_model_name,\n",
        "    run_opts={\"device\": device},\n",
        "    savedir=os.path.join(\"/tmp\", \"speechbrain_spkrec_xvect\"),\n",
        ")\n",
        "\n",
        "def create_speaker_embedding(waveform):\n",
        "    with torch.no_grad():\n",
        "        wav = waveform if torch.is_tensor(waveform) else torch.tensor(waveform)\n",
        "        wav = wav.float()\n",
        "\n",
        "        # expected shape: [batch, time]\n",
        "        if wav.ndim == 1:\n",
        "            wav = wav.unsqueeze(0)\n",
        "        elif wav.ndim != 2:\n",
        "            raise ValueError(f\"Expected 1D or 2D waveform, got shape {tuple(wav.shape)}\")\n",
        "\n",
        "        wav = wav.to(device)\n",
        "\n",
        "        emb = speaker_model.encode_batch(wav)                 # usually [B, 1, D]\n",
        "        emb = torch.nn.functional.normalize(emb, dim=2)\n",
        "        return emb.squeeze().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "hBS8Z_D3KlZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example = processor(\n",
        "        text=example[\"normalized_text\"],\n",
        "        audio_target=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "    example[\"labels\"] = example[\"labels\"][0]\n",
        "\n",
        "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "xTQ3u_rYNN_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_example = prepare_dataset(dataset[0])\n",
        "list(processed_example.keys())"
      ],
      "metadata": {
        "id": "8BEy6cPENQSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_example[\"speaker_embeddings\"].shape"
      ],
      "metadata": {
        "id": "fkWDDZ3-NSg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "id": "bMWtsdqhNUXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_not_too_long(input_ids):\n",
        "    input_length = len(input_ids)\n",
        "    return input_length < 200\n",
        "\n",
        "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "-PHPC1W2SaTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "Qs4EvwGjTFvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TTSDataCollatorWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
        "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
        "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
        "\n",
        "        batch = processor.pad(\n",
        "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
        "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
        "        )\n",
        "\n",
        "        del batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        if model.config.reduction_factor > 1:\n",
        "            target_lengths = torch.tensor(\n",
        "                [len(feature[\"input_values\"]) for feature in label_features]\n",
        "            )\n",
        "            target_lengths = target_lengths.new(\n",
        "                [\n",
        "                    length - length % model.config.reduction_factor\n",
        "                    for length in target_lengths\n",
        "                ]\n",
        "            )\n",
        "            max_length = max(target_lengths)\n",
        "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
        "\n",
        "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "H-_tIENzTJpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = TTSDataCollatorWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "orAkXD9ZUdV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5ForTextToSpeech\n",
        "\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "-1tATjkqUkK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "model.generate = partial(model.generate, use_cache=True)"
      ],
      "metadata": {
        "id": "VEwAo9tPUrqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"speecht5_finetuned_russian_speech\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=100,\n",
        "    max_steps=500,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    label_names=[\"labels\"],\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "82wBYuN4U10b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=processor,\n",
        ")\n"
      ],
      "metadata": {
        "id": "qUinNO8OVgDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "46L_TpawVv1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "1E55p6DqkqNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SpeechT5ForTextToSpeech.from_pretrained(\n",
        "    \"speecht5_finetuned_russian_speech\"\n",
        ")"
      ],
      "metadata": {
        "id": "34eFcXaikq9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[\"test\"][304]\n",
        "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)"
      ],
      "metadata": {
        "id": "5AW5OScjmxD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Привет, меня зовут Ералы, я студент университета\""
      ],
      "metadata": {
        "id": "qm2PL4P-m89I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "RU_DIGITS = {\n",
        "    \"0\": \"ноль\", \"1\": \"один\", \"2\": \"два\", \"3\": \"три\", \"4\": \"четыре\",\n",
        "    \"5\": \"пять\", \"6\": \"шесть\", \"7\": \"семь\", \"8\": \"восемь\", \"9\": \"девять\",\n",
        "}\n",
        "\n",
        "def replace_numbers_with_words(text: str) -> str:\n",
        "    # replaces single digits only (simple but safe)\n",
        "    return re.sub(r\"\\d\", lambda m: RU_DIGITS[m.group(0)], text)\n"
      ],
      "metadata": {
        "id": "iOGT9OMSnzq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_text(text):\n",
        "    for src, dst in replacements:\n",
        "        text = text.replace(src, dst)\n",
        "    return text"
      ],
      "metadata": {
        "id": "v1dPEWcKnQcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_text = replace_numbers_with_words(text)\n",
        "cleaned_text = cleanup_text(converted_text)\n",
        "final_text = normalize_text(cleaned_text)\n",
        "final_text"
      ],
      "metadata": {
        "id": "2i01acRHne9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(text=final_text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "VLr8nN2KoQn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5HifiGan\n",
        "\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)"
      ],
      "metadata": {
        "id": "VEoGL846oUU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "import soundfile as sf\n",
        "\n",
        "Audio(speech.numpy(), rate=16000)\n",
        "sf.write('output.wav', speech.numpy(), 16000)"
      ],
      "metadata": {
        "id": "SJzAw3PnoZ6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}